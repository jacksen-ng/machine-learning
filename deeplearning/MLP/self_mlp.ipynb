{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this section, I will try to implement a simple MLP and train it on the MNIST dataset.\n",
    "- I will use the `pytorch` library to implement the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I use the `FashionMNIST` dataset for this experiment.\n",
    "- Split the dataset into training, validation and testing sets. Training set is 70% of the dataset, validation set is 20% and testing set is 10%.\n",
    " - batch size is 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data transform\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "#load data\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=data_transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=data_transform)\n",
    "\n",
    "# split data\n",
    "train_dataset, val_dataset = random_split(train_dataset, [int(len(train_dataset)*0.7), int(len(train_dataset)*0.3)])\n",
    "\n",
    "# create data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this simple MLP, I will use my macbook pro M3 to run the experiment. But after the experiment, I will try to run it on the GPU.\n",
    "- I will use the `Adam` optimizer and the `CrossEntropyLoss` loss function.\n",
    "- I will train the model for 10 epochs.\n",
    "- I will use the `accuracy` metric to evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch size:  64\n",
      "image shape:  1 28 28\n",
      "label shape:  64\n",
      "tensor([7, 5, 3, 1, 9, 1, 9, 1, 0, 6, 8, 7, 5, 9, 3, 1, 2, 9, 4, 5, 4, 7, 8, 4,\n",
      "        6, 0, 5, 3, 2, 5, 3, 2, 6, 3, 0, 9, 5, 2, 9, 7, 9, 9, 3, 7, 0, 4, 4, 1,\n",
      "        6, 0, 8, 3, 7, 4, 3, 8, 4, 8, 7, 2, 3, 1, 7, 4])\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgAHoDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/oorqvCfw/1vxcRLaRJDYh9j3czYUEdcDqx57DHqRQBytFfTNv8ACnwfaaMdOksPtMrj57l3IlzjGQwPycgnHIycHNeefEz4b6T4W8PQarpf2pD9ojt5I5ZlkBBRjvGACOV/XtQB5TWlpOnWeoPL9s1i102OMA7p0kcvnsoRTn8cVm0cigDsrLwn4f1DULaxtfFUk805IzHpx2pgEksWcHAAJOB2rkbiIQXEsQbeEcruxjODjOK7XwPpN5JoevanbtHbyNCljBcXDBIwZGHmEMerBB0GT8/TkZ53xBo8Oi3MFul+LyVo98jJEyIDkj5S3LDjqQv0oAyK9i0T4feHr74QHxTPby/b47C7c/v2CO6M4ViPUbRgDAPcGvHa+nvCuhJJ8B4tMDDdd6XcS7u2+Teyn8DtH4UAfMNFFFAHV/DvwxZ+LPFaadqEk6WiwSTSmBgrnaOACQQPmI7V6nqnwX8LxaPfTWMmqi6htnaHzbmNlLhSRuAjB6+hFcP8EpAvxBER/wCWtnMv5AN/7LXvc2pR3p8T2e2Jf7NcRfIMEq1ssmW99xcfRRQB8fUUUUAek/DqHwIbAy+IDnUhMcCeNniVMDHyrnOeclgRxXr0PjPwtCEjg1q12ou2OOOCY4HoFEf/ANavliigD6I134v6BpDNHZwXF/c+hBiQccZJy3pxj8q8s8YfEvV/F9kLGeOC3sw4k8qFMZIzjJJJPX1/CuKooA9C8C+AjqN6LvVhAtsoUxRm4T94SMgkZOVx24yfoRXV/wDCMeEdW0C4vdR0DUdIvIiqiEAQPI3pH1Vl7Ftox+WeC0XxNpcFjDa6jaOWiXZ5ioH3D8wR6d62z4t0KNWe2upopGboIMgr6H3oA6RrlGijR4kigiHlwwxcLCnogz09T1JyTkkmvMvG0iv4jdF4EcSLjOcEjd/WuoHjLR2bBuZQPU25x+ODXDa9ex6hrl3dQsWjkf5SRgkAADj8KAM6vq7wPIZPg7pjjPGlzL+QlH9K+Ua998IfEzwppPwysdIvtRdL6O1mheFLaRipYyY5xtPDDoe/1wAeBUUUUAdv8IXCfFHR9xwGE6fiYJAP1Nen6RdkeKvitE7HBXzMjsFWVf5NXiPhPVk0LxbpWqS7vKtrlHk2jJ2Z+bA78ZrsofHOlQ+KPHV7vnNrrNpPHanyuWdiNu4Z4HJoA81ooooA/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=122x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the images that I will use for this experiment\n",
    "images, labels = next(iter(train_loader))\n",
    "c, h, w = images[0].shape\n",
    "print(\"minibatch size: \", len(images))\n",
    "print(\"image shape: \", c, h, w)\n",
    "print(\"label shape: \", len(labels))\n",
    "\n",
    "# check the label\n",
    "print(labels)\n",
    "\n",
    "# check the image\n",
    "img = torchvision.utils.make_grid(images[:4])\n",
    "img = transforms.functional.to_pil_image(img)\n",
    "display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "input_size = 28*28\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__() # initialize the parent class\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = MLP().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6026495695114136\n",
      "Epoch 2/10, Loss: 0.3346196115016937\n",
      "Epoch 3/10, Loss: 0.37666141986846924\n",
      "Epoch 4/10, Loss: 0.42103877663612366\n",
      "Epoch 5/10, Loss: 0.23780274391174316\n",
      "Epoch 6/10, Loss: 0.40607205033302307\n",
      "Epoch 7/10, Loss: 0.3137008249759674\n",
      "Epoch 8/10, Loss: 0.2323542684316635\n",
      "Epoch 9/10, Loss: 0.8264886736869812\n",
      "Epoch 10/10, Loss: 0.2279001772403717\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 82.64%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Accuracy of the model on the 10000 test images: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- right now, the accuracy is 82.64%\n",
    "- It's not bad, but it's not good enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
